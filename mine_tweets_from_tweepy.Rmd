---
title: "Parsing and analyzing tweets mined with Tweepy"
author: "Kris Shaffer"
output:
  html_document: default
  html_notebook: default
---

This notebook walks through the process of parsing and analyzing tweets collected from the Twitter API with Tweepy.

First, load some libraries.

```{r}
library(tidyverse)
library(tidytext)
library(lubridate)
library(stringr)
library(jsonlite)
```

## Load the tweets

First, declare the source of your tweets data — i.e., the search string that you used to source the tweets. This will be loaded into the titles for the plots below.

```{r}
# tweet archive source (for inclusion in plot titles)
source_text <- ''
```

Now import the data. If you have the tweets in a single CSV file, use this code:

```{r}
# import tweets from a single CSV file
csv_file_name <- ''
tweets <- read_csv(csv_file_name, col_types = 'cccTccccciccliiccccc')
```

If you have multiple files — for example, if you used `twitter_search.py` to search Twitter history and `twitter_stream.py` to collect a live stream of tweets, each of which exported to their own CSV file — use this code:

```{r}
# import tweets from two CSV files (search and stream)
# search_file <- ''
# stream_file <- ''
# tweets <- read_csv(search_file, col_types = 'cccTccccciccliiccccc') %>%
#   full_join(read_csv(stream_file, col_types = 'cccTccccciccliiccccc')) %>%
#   unique()
```

Note the `col_types` field. This prevents R from converting the long numerical tweet/user ID fields into rounded scientific notation, thus breaking the data. It imports those fields as characters instead.

## Some functions

Before exploring the data, we need a few functions. These functions will extract the `expanded_url` field from the tweet's `entities` object, and then extract the root domains from those urls.

```{r}
# functions to extract URLs from JSON entities_str cell
get_expanded_url <- function(cell) {
  if (!is.na(cell)) {
    try(url <- fromJSON(gsub("'", '"', cell)))
    if (exists('url') & length(url$url) > 0) {
      return(url$expanded_url)
    } else {
      return(NA)
    }
  } else {
    return(NA)
  }
}

# function to extract domains from URLs
extract_domain <- function(url) {
  return(gsub('www.', '', unlist(strsplit(unlist(strsplit(as.character(url), '//'))[2], '/'))[1]))
}
```

## Extract URLs from tweets

Now we can extract the URLs from tweets and create a second tibble, `tweets_with_urls` (alongside `tweets`) that contains only tweets with URLs, with one record per URL (which may mean more than one record per tweet). Keep this in mind for subsequent analyses: use `tweets_with_urls` when analyzing urls, and `tweets` when analyzing other tweet content.

```{r}
# extract urls and domains from tweets
tweets_with_urls <- tweets %>%
  mutate(url = sapply(entities_urls, get_expanded_url),
         domain = mapply(extract_domain, url)) %>%
  filter(!is.na(url)) %>%
  unnest()
```

## Count URLs and domains

Now we can count (and store) the number of times each unique URL and domain are linked from tweets in the corpus. We'll store these results in `url_list` and `domain_list` and save them to files.

```{r}
# count URLs frequency of occurrence
url_list <- tweets_with_urls %>%
  group_by(url) %>%
  summarize(count = n()) %>%
  select(url, count) %>%
  filter(!grepl('https://t.co/', url),
         !grepl('https://twitter.com/', url)) %>%
  arrange(desc(count))

# count the frequency of a domain's occurrence
domain_list <- url_list %>%
  mutate(domain = mapply(extract_domain, url)) %>%
  group_by(domain) %>%
  summarize(domain_count = sum(count)) %>%
  arrange(desc(domain_count))

# write domain info to files
write_csv(domain_list, '')
write_csv(url_list, '')

url_list
domain_list
```

## Analyze and plot some summary statistics

There's one more thing to calculate before adding to the titles of our plots: the earliest date in the corpus.

```{r}
# plot the most common domains in the corpus
min_date <- tweets %>%
  select(created_at) %>%
  mutate(date = ymd(substring(created_at, 1, 10))) %>%
  .$date %>%
  min()
```

Now we can plot the most common domains shared in the corpus. 

```{r}
domain_list %>%
  filter(domain != 'twitter.com' # filter out internal tweet shares
         # domain_count > 10 # use this to limit the plot only to the most frequently shared domains
         ) %>%
  mutate(domain = reorder(domain, domain_count)) %>%
  ggplot(aes(domain, domain_count, fill = domain)) +
  geom_bar(stat = 'identity') +
  xlab(NULL) +
  ylab(paste('domain count (since ', 
             min_date,
             ')', sep = '')) +
  ggtitle(paste('Most common domains (other than twitter.com) in tweets containing', source_text)) +
  theme(legend.position="none") +
  coord_flip()
```

Or the most commonly used tweet sources.

```{r}
tweets %>%
  group_by(source) %>%
  summarize(count = n()) %>%
  mutate(source = reorder(source, count)) %>%
  ggplot(aes(source, count, fill = source)) +
  geom_bar(stat = 'identity') +
  xlab(NULL) +
  ylab(paste('source count (since ', 
             min_date,
             ')', sep = '')) +
  ggtitle(paste('Most common clients/apps in tweets containing', source_text, '\n',
                '(looking for clients that are commonly used for automation)')) +
  theme(legend.position="none") +
  coord_flip()
```

## Words and bigrams

We can use the `tidytext` package to tokenize the tweet text by word or n-gram, and then analyze/visualize that content. Let's start by tokenizing it by words and by bigrams, storing the results in `tidy_tweets` and `tidy_bigrams`, respectively. You can comment out the lines that remove hashtags and handles, if you want to include them.

```{r}
# extract words from tweets (excepting RTs), make tidy
reg_words <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg_words) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

# bigrams
tidy_bigrams <- tweets %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg_words) %>%
  mutate(next_word = lead(word)) %>%
  filter(!word %in% stop_words$word, # remove stop words
         !next_word %in% stop_words$word, # remove stop words
         substr(word, 1, 1) != '@', # remove user handles to protect privacy
         substr(next_word, 1, 1) != '@', # remove user handles to protect privacy
         substr(word, 1, 1) != '#', # remove hashtags
         substr(next_word, 1, 1) != '#',
         str_detect(word, "[a-z]"), # remove words containing ony numbers or symbols
         str_detect(next_word, "[a-z]")) %>% # remove words containing ony numbers or symbols
  filter(id_str == lead(id_str)) %>% # needed to ensure bigrams to cross from one tweet into the next
  unite(bigram, word, next_word, sep = ' ')
```

Now we can visualize the most common non-stop words (including handles and hashtags, but not elements from URLs).

```{r}
tidy_tweets %>%
  count(word, sort=TRUE) %>%
  filter(
         # substr(word, 1, 1) != '#', # omit hashtags
         # substr(word, 1, 1) != '@', # omit Twitter handles
         n > 20 # set a minimum number of occurrences to include in the visualization
         ) %>% 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = word)) +
  geom_bar(stat = 'identity') +
  xlab(NULL) +
  ylab(paste('Word count (since ', 
             min_date,
             ')', sep = '')) +
  ggtitle(paste('Most common words in tweets containing', source_text)) +
  theme(legend.position="none") +
  coord_flip()
```

And the most common bigrams (not including handles, hashtags, or elements from URLs).

```{r}
tidy_bigrams %>%
  count(bigram, sort=TRUE) %>%
  filter(n >= 5) %>% # set a minimum number of occurrences to include in the visualization
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(bigram, n, fill = bigram)) +
  geom_bar(stat = 'identity') +
  xlab(NULL) +
  ylab(paste('bigram count (since ', 
             min_date,
             ')', 
             sep = '')) +
  ggtitle(paste('Most common bigrams in tweets containing', source_text, '(excluding hashtags and handles)')) +
  theme(legend.position="none") +
  coord_flip()
```
